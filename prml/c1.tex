\chapter{Introduction}\label{c1}
\section{Notation and definitions}\label{c1s1}
We will follow the contemporary mathematical style of using unadorned symbols for vectors and
matrices. The nature of a symbol will be determined by its definition and not appearance. For
example, $x$ could be a real number or a point in $\sor^n$ depending on the context. Random
variables will be denoted by capital letters and their realizations by small letters.

If $f$ is a function of $n$ variables, $x_1, \ldots, x_n$ then its gradient is defined as
\begin{equation}\label{c1s1e1}
Df := \left(\frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n}\right).
\end{equation}
The hessian of $f$ is
\begin{equation}\label{c1s1e2}
(D^2 f)_{ij} := \frac{\partial^2 f}{\partial x_i \partial x_j}.
\end{equation}

\section{Bayesian curve fitting}\label{c1s2}
The problem consists of using the data $\{(x_i, t_i) : 1 \le i \le n\}$ to find the value
of $t$ when a new $x$ is given. We shall fit the data using the polynomial
\begin{equation}\label{c1s2e1}
y(x, w) = \sum_{i=0}^M w_i x^i.
\end{equation}
The polynomial coefficients $w_0, \ldots, w_M$ can be considered to be components of a 
vector $w \in \sor^M$. We assume that for a given value of $x_i$, the target $t_i$ has a 
gaussian distribution with mean $y(x, w)$ and precision $\beta$ (or variance $\beta^{-1}$).
Thus,
\begin{equation}\label{c1s2e2}
p(t_i | x_i, w, \beta) = \mathcal{N}(t | y(x_i, w), \beta^{-1/2}).
\end{equation}
We will use the training data $\{(x_i, t_i): 1 \le i \le N\}$ to estimate the values of $w$
and $\beta$ using maximizing the likelihood function. If the data are assumed to be drawn
independently from the distribution of equation \eqref{c1s2e2} then the likelihood function
is
\[
p(t | x, w, \beta) = \prod_{i=1}^N p(t_i | x_i, w, \beta) = \prod_{i=1}^N \mathcal{N}(y(x_i, w), \beta^{-1/2})
\]
where $x, t \in \sor^n$, $w \in \sor^{M+1}$ and $y$ is given by \eqref{c1s2e1}. The log-likelihood
function is
\begin{equation}\label{c1s2e3}
\ln p(t|x,w,\beta) = -\frac{\beta}{2}\sum_{i=1}^N(y(x_i, w) - t_i)^2 + \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi).
\end{equation}
Maximizing $\ln p$ with respect to $w$ is the same as minimizing the quantity,
\begin{equation}\label{c1s2e4}
\sum_{i=1}^N(y(x_i, w) - t_i)^2.
\end{equation}
This quantity is the sum of squares of residuals and the problem of finding optimal $w$ reduces
to that of ordinary least squares regression. In order to find $\beta$ that maximizes $\ln p$
we differentiate the equation with respect to $\beta$ to get
\[
\frac{\partial}{\partial\beta}\ln p(t|x,w,\beta) = -\frac{1}{2}\sum_{i=1}^N(y(x_i, w) - t_i)^2 + \frac{N}{2\beta}
\]
The derivative vanishes when $\beta = \beta_{ML}$, where
\begin{equation}\label{c1s2e5}
\frac{1}{\beta_{ML}} = \frac{1}{N}\sum_{i=1}^N(y(x_i, w) - t_i)^2.
\end{equation}
If we are now given a new $x$, say $x_{N+1}$ then the corresponding target value $t_{N+1}$ has
a distribution
\[
p(t_{N+1} | x_{N+1}, w_{ML}, \beta_{ML}) = \mathcal{N}(t_{N+1}| y(x_{N+1}, w_{ML}), \beta_{ML}^{-1}).
\]

We now choose a prior distribution for $w$. Let it be a gaussian with mean zero and variance-covariance
matrix $\alpha^{-1}I$, where $I$ is an $(M+1) \times (M+1)$ identity matrix. Thus,
\begin{equation}\label{c1s2e6}
p(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1}I) = \left(\frac{\alpha}{2\pi}\right)^{(M+1)/2}\exp\left(-\frac{\alpha}{2}w^Tw\right).
\end{equation}
The posterior distribution for $w$ given $(x_i, t_i), \alpha, \beta$ is
\begin{equation}\label{c1s2e7}
p(w|x, t, \alpha, \beta) \propto p(t|x,w,\beta)p(w|\alpha).
\end{equation}
We can now find $w$ suitable for the data by maximizing the right hand side. The posterior probability
on the right hand side is conditioned on the data $(x, t)$, the hyperparameter $\alpha$ and the maximum
likelihood estimate $\beta$. Taking a logarithm of both sides of equation \eqref{c1s2e7},
\[
\ln p(w|x,t,\alpha,\beta) = \ln p(t|x,w,\beta) + \ln p(w|\alpha) + \ln K,
\]
where $K$ is the constant of proportionality in \eqref{c1s2e7}. From equations \eqref{c1s2e3} and
\eqref{c1s2e6} we get
\[
\ln p(w|x,t,\alpha,\beta) = -\frac{\beta}{2}\sum_{i=1}^N(y(x_i, w) - t_i)^2 + \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi) - \frac{\alpha}{2}w^Tw + \ln K.
\]
Maximizing $\ln p(w|x,t,\alpha,\beta)$ with respect to $w$ is equivalent to minimizing the function
\[
\frac{\beta}{2}\sum_{i=1}^N(y(x_i, w) - t_i)^2 + \frac{\alpha}{2}w^Tw.
\]
This expression is the regularized least square error of equation (1.4) of the book. Equations (1.68) to (1.72)
will be derived in chapter 3.

\section{Problems}\label{c1p}
\begin{enumerate}
\item In this problem, $w = (w_0, \ldots, w_M) \in \sor^{M+1}$, $x, y \in \sor$. Likewise, the $n$
observations $y_1, \ldots, y_n$ for each one of $x_1, \ldots, x_n$ are also real numbers. $y$
is modelled as a polynomial in $x$. That is,
\begin{equation}\label{c1pe1}
y = y(x, w) = \sum_{i=0}^Mw_ix^i.
\end{equation}
The model error is given by
\begin{equation}\label{c1pe2}
E(w) = \frac{1}{2}\sum_{j=1}^N \left(y(x_j, w) - t_j\right)^2,
\end{equation}
where $t_j$ is the true value of $y$ at $x_j$. The goal of the exercise is to choose $w$ such 
that for a given set of observations $(x_j, t_j)$, $E$ is minimal. Therefore, we find the
gradient of $E$,
\[
DE = \sum_{j=1}^N\left(y(x_j, w) - t_j\right)Dy.
\]
The gradient of $y$ with respect to $w$ is
\[
Dy = \sum_{i=0}^M x^i,
\]
so that
\[
DE = \sum_{j=1}^N\left(y(x_j, w) - t_j\right)\sum_{i=0}^M x_j^i.
\]
The condition for an extremum is $DE = 0$. The vector $DE$ is zero if and only if each one
of its components is zero.
\[
\sum_{j=1}^N\left(\sum_{k=0}^M w_k x_j^{k+i} - t_jx_j^i\right) = 0, \forall i = 0, \ldots, M
\]
We can simplify it as
\[
\sum_{k=0}^M \sum_{j=1}^N w_k x_j^{k+i} = \sum_{i=0}^M\sum_{j=1}^N t_jx_j^i.
\]
Let,
\begin{eqnarray*}
T_i &=& \sum_{j=1}^N t_jx_j^i \\
A_{ik} &=& \sum_{j=1}^N x_j^{k+i}
\end{eqnarray*}
so that the condition from an extremum becomes
\[
\sum_{k=0}^M A_{ik}w_k = T_i
\]

\item The regularized error function is
\begin{equation}\label{c1pe3}
E(w) = \frac{1}{2}\sum_{j=1}^N \left(y(x_j, w) - t_j\right)^2 + \frac{\lambda}{2}\norm{w}^2.
\end{equation}
Its gradient is
\[
DE = \sum_{j=1}^N\left(y(x_j, w) - t_j\right)Dy + \lambda w,
\]
where we have used the fact that 
\[
D\norm{w}^2 = \left(\frac{\partial}{\partial w_0}\sum_{i=0}^M w_i^2, \ldots, \frac{\partial}{\partial w_M}\sum_{i=0}^M w_i^2\right) = 2w.
\]
The vector $DE = 0$ if and only if all its components are zero. Therefore,
\[
\sum_{j=1}^N\left(\sum_{k=0}^M w_k x_j^{k+i} - t_jx_j^i\right) + \lambda w_i = 0, \forall i = 0, \ldots, M
\]
Using the definitions of $A_{ik}$ and $T_i$ introduced in the previous problem, the condition
for extremum becimes
\[
\sum_{k=0}^M A_{ik}w_k = T_i - \lambda w_i.
\]

\item We are given that $p(r) = 0.2, p(b) = 0.2, p(b) = 0.6$. The conditional probabilities are
$p(a|r) = 0.3, p(o|r) = 0.4, p(l|r) = 0.3$, $p(a|b) = 0.5, p(o|b) = 0.5$ and $p(a|g) = 0.3, p(o|g)
= 0.3, p(l|g) = 0.6$.

Now, 
\[
p(a) = p(a|r)p(r) + p(a|b)p(b) + p(a|g)p(g) = 0.34.
\]

If the selected fruit is orange, the probability that it came from the green box is $p(g|o)$. Using
Bayes theorem,
\[
p(g|o) = \frac{p(g, o)}{p(o)} = \frac{p(o|g)p(g)}{p(o)}.
\]
The denominator is calculated as
\[
p(o) = p(o|r)p(r) + p(o|b)p(b) + p(o|g)p(g) = 0.36
\]
Therefore,
\[
p(g|o) = \frac{0.3 \times 0.6}{0.36} = 0.5.
\]

\item Consider equation (1.27) of the book. Instead of writing it with modulus, we write it as
\[
p_y(y) = \pm p_x(g(y))g^\prime(y),
\]
where we chose the sign to make $p_y \ge 0$ for all $y$. The extremum of this function is found
using the relation
\begin{equation}\label{c1pe4}
p_y^\prime(y) = \pm\left(\frac{dp_x}{dg}\left(g^\prime(y)\right)^2 + p_x(g(y))g^{\prime\prime}(y)\right)
\end{equation}
and equating it to zero. If instead of a probability density, we had an ordinary
function $h(y) = f(g(y))$. Then $h^\prime(y) = df/dg g^\prime(y)$ and the condition for extremum
would be $df/dg = 0$ if $g^\prime(y) \ne 0$. The value of $y$ obtained using this relation
is related to the $x$ found using $f^\prime(x) = 0$ is precisely $x = g(y)$. However, it
is not so because of the second term on the right hand side of equation \eqref{c1pe4}.

However, if $g$ is a linear function of $y$ then its second derivative vanishes and the two
equations become similar.

\item We start with the definition $\var(f) = \ev(f(x) - \ev(f(x)))^2 = \ev(f^2(x) - 2f(x)\ev(f(x)) + (\ev(f(x)))^2)
= \ev(f^2(x)) - 2\ev(f(x))\ev(f(x)) + (\ev(f(x))^2) = \ev(f^2(x)) - (\ev(f(x)))^2$, where we 
have used the fact that $\ev(x)$ is a constant and $\ev1(ax) = a\ev(x)$ for any constant $a$.

\item The covariance of two random variables $X$ and $Y$ is given by $\cov(X, Y) = \ev(XY) - \ev(X)\ev(Y)$.
If $p$ is the joint probability density of $X$ and $Y$ and if $p_X$ and $p_Y$ are the respective marginal 
probability densities then,
\begin{eqnarray*}
\ev(XY) &=& \iint p(X, Y)Xy dXdy \\
\ev(X)  &=& \int p_X(X) X dX \\
\ev(Y)  &=& \int p_Y(Y) y dy
\end{eqnarray*}
If $X$ and $Y$ are independent $p(X, Y) = p_{X|Y}(X|Y)p_Y(Y) = p_X(X)p_Y(Y)$ and the $\ev(XY)$ simplifies
to
\[
\ev(XY) = \int p_X(X)XdX \int p_Y(Y)ydy.
\]
The covariance of independent random variables thus vanishes.

\item The proof of normality of the gaussian distribution depends on the integral of $\exp(-ax^2)$
over the real line. Here $a$ is a real constant and $x$ a real variable. Let
\[
I = \int_\sor e^{-ax^2}dx = \int_\sor e^{-ay^2}dy.
\]
Therefore,
\[
I^2 = \iint_{\sor\times\sor}e^{-a(x^2+y^2)}dxdy.
\]
Change the coordinates from $(x, y)$ to $(r, \theta)$, where $x = r\cos\theta$ and $y = r\sin\theta$.
The jacobian of transformation is $r$ so that
\[
I^2 = \int_0^\infty\int_0^{2\pi}e^{-ar^2}rdrd\theta = 2\pi\int_0^\infty e^{-ar^2}rdr.
\]
Introduce the variable $s = r^2$ so that $ds = 2rdr$ and hence,
\[
I^2 = \pi\int_0^\infty e^{-as}ds = \frac{\pi}{a}
\]
so that
\begin{equation}\label{c1pe5}
\int_\sor e^{-ax^2}dx = \sqrt{\frac{\pi}{a}}.
\end{equation}
The normal density is
\[
\mathcal{N}(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\]
so that
\[
\int_\sor\mathcal{N}(x|\mu,\sigma)dx = \frac{1}{\sqrt{2\pi\sigma^2}}\int_\sor\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)dx.
\]
Introduce the variable $u = (x - \mu)/(\sigma\sqrt{2})$ so that $dx = \sigma\sqrt{2}du$ and the limits of the integral
remain unchanged. Thus,
\[
\int_\sor\mathcal{N}(x|\mu,\sigma)dx = \frac{1}{\sqrt{\pi}}\int_\sor e^{-u^2}du = 1.
\]

\item Let $X$ be a random variable with distribution $\mathcal{N}(x|\mu,\sigma)$. Then its expectation
is
\[
\ev(X) = \int_\sor\mathcal{N}(x|\mu,\sigma)xdx.
\]
Introduce the variable
\begin{equation}\label{c1pe6}
u = \frac{x - \mu}{\sigma\sqrt{2}}
\end{equation}
so that $x = \mu + u\sigma\sqrt{2}$, $dx = \sigma\sqrt{2}du$ and the limits of
the integral remain unchanged. Then,
\[
\ev(X) = \frac{1}{\sqrt{\pi}}\int_\sor e^{-u^2}(\mu + u\sigma\sqrt{2})du = \mu + \sigma\sqrt{\frac{2}{\pi}}\int_\sor ue^{-u^2}du.
\]
The second integral is zero because the integrand is an odd function of $u$ and the limits of the
integral are symmetric around the origin.

The variance of a normally distributed random variable is $\var(X) = \ev(X^2) - (\ev X)^2 
= \ev(X^2) - \mu^2$, where we have used the previous exercise. Thus, in order to get the variance
we just need the expectation of $X^2$. It is
\[
\ev(X^2) = \int_\sor\mathcal{N}(x|\mu,\sigma)x^2dx.
\]
We once again change the variable of integration to $u$ defined in equation \eqref{c1pe6} so that
\begin{equation}\label{c1pe7}
\ev(X^2) = \frac{1}{\sqrt{\pi}}\int_\sor e^{-u^2}(\mu^2 + 2\sqrt{2}\mu\sigma u + 2\sigma^2u^2)du.
\end{equation}
The right hand side is the sum of three integrals of which the first one evaluates to $\mu^2$ and
the second one to $0$. The third one is
\begin{equation}\label{c1pe8}
I = \frac{2\sigma^2}{\sqrt{\pi}}\int_\sor e^{-u^2}u^2du.
\end{equation}
In order to evaluate this integral we differentiate equation \eqref{c1pe5} with respect to $a$ to
get
\[
-\int_\sor e^{-ax^2}x^2dx = -\frac{1}{2}\frac{\pi}{a^{3/2}}
\]
or
\begin{equation}\label{c1pe9}
\int_\sor e^{-ax^2}x^2dx = \frac{\sqrt{\pi}}{2}\frac{1}{a^{3/2}}.
\end{equation}
Using equation \eqref{c1pe9} in \eqref{c1pe8} we get $I = \sigma^2$. Therefore, equation \eqref{c1pe7}
becomes
\[
\ev{X^2} = \mu^2 + \sigma^2
\]
from which it immediately follows that $\var{X} = \sigma^2$.

\item The mode of a density function is its extremum. In the case of univariate normal distribution,
\[
\frac{d\mathcal{N}}{dx} = \frac{-1}{\sqrt{2\pi\sigma^2}}\frac{x - \mu}{2\sigma^2}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right).
\]
The right hand side vanishes at $x = \mu$. To confirm that the extremum is a maximum, we need the
second derivative.
\begin{eqnarray*}
\frac{d^2\mathcal{N}}{dx^2} &=& \frac{-1}{2\sigma^3\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \\
 & & + \frac{1}{\sqrt{2\pi\sigma^2}}\frac{(x - \mu)^2}{4\sigma^4}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \\
 &=& \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\left[\frac{(x - \mu)^2}{4\sigma^4} - \frac{1}{2\sigma^2}\right]
\end{eqnarray*}
The second derivative is negative at $x = \mu$ confirming that the extremum is indeed a maximum.

The multivariate normal distribution is
\[
\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{n/2}\abs{\Sigma}^2}\exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right),
\]
where $x, \mu \in \sor^n$ and $\Sigma$ is the $n \times n$ variance-covariance matrix. Its gradient is
\[
D\mathcal{N} = \frac{-1}{(2\pi)^{n/2}\abs{\Sigma}^2}\exp\left(-\frac{(x - \mu)^T\Sigma^{-1}(x - \mu)}{2}\right)\left(\frac{\Sigma^{-1}(x - \mu)}{2} + \frac{(x - \mu)^T\Sigma^{-1}}{2}\right).
\]
As $\Sigma$ is a symmetric matrix, so is its inverse and hence $\Sigma^{-1}(x - \mu) = (x - \mu)^T\Sigma^{-1}$ and hence
\[
D\mathcal{N} = -\frac{1}{{2\pi}^{n/2}\abs{\Sigma}^2}\exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)\Sigma^{-1}(x - \mu).
\]
The gradient vanishes when $x = \mu$. In order to examine the nature of the maximum, we need the
hessian of the function. 
\[
D^2\mathcal{N} = \frac{1}{{2\pi}^{n/2}\abs{\Sigma}^2}\exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right)\left((x - \mu)^T(\Sigma^{-1})^T\Sigma^{-1}(x - mu) - \Sigma^{-1}\right).
\]
At $x = \mu$, the hessian becomes
\[
D^2\mathcal{N}(x = \mu) = -\frac{\Sigma^{-1}}{{2\pi}^{n/2}\abs{\Sigma}^2}.
\]
The variance-covariance matrix is positive semi-definite. So it is its inverse and hence $\tr(\Sigma^{-1}) \ge 0$.
As a result, $\tr(D^2\mathcal{N}) < 0$ at $x = \mu$, making it a minimum point.

\item As $X$ and $Z$ are independent random variables, $p(X, Z) = p_{X|Z}(x)p_Z(z) = p_X(x)p_Z(z)$. 
\begin{eqnarray*}
\ev(X+Z) &=& \iint p_{X+Z}(x+z)dxdz \\
         &=& \iint p_X(x)p_Z(z)(x+z)dxdz \\
         &=& \int p_X(x)dx\int p_Z(z)dz + \int p_X(x)dx\int p_Z(z)zdz \\
         &=& \ev{X} + \ev{Z}
\end{eqnarray*}
Similarly,
\begin{eqnarray*}
\ev(X+Z)^2 &=& \ev(X^2 + 2XZ + Z^2) \\
           &=& \ev(X^2) + 2\ev(XZ) + \ev(Z^2)
\end{eqnarray*}
Now,
\[
\ev(XZ) = \iint xzp(x,z)dxdz = \int p_X(x)dx\int p_Z(z)dz = \ev(X)\ev(Z)
\]
so that
\[
\ev(X+Z)^2 = (\ev(X) + \ev(Z))^2
\]
and hence $\var(X + Z) = \ev(X^2) - (\ev(X))^2 + \ev(Z^2) - (\ev(Z))^2 = \var(X) + \var(Z)$.

\item The log-likelihood function for the gaussian is
\[
\ln p(x | \mu, \sigma^2) = -\frac{1}{2\sigma^2}\sum_{n=1}^N(x_n - \mu)^2 - \frac{N}{2}\ln\sigma^2 - \frac{N}{2}\ln 2\pi,
\]
where $x = (x_1, \ldots, x_n) \in \sor^n$ is the vector of realizationf of $n$ independent and identically gaussian 
distributed random variables with parameters $\mu$ and $\sigma$ respectively. Then,
\begin{eqnarray*}
\frac{\partial}{\partial\mu}\ln p(x | \mu, \sigma^2) &=& \frac{1}{\sigma^2}\sum_{n=1}^N (x_n - \mu) \\
\frac{\partial}{\partial\sigma}\ln p(x | \mu, \sigma^2) &=& \frac{1}{\sigma^3}\sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{\sigma} 
\end{eqnarray*}
Equating the derivatives to zero we get
\begin{eqnarray*}
\mu_{ML} &=& \frac{1}{N}\sum_{n=1}^N x_n \\
\sigma_{ML} &=& \left(\frac{1}{N}\sum_{n=1}^N(x_n - \mu)^2\right)^{1/2}
\end{eqnarray*}

\item Let $X_1, \ldots, X_n$ be $n$ i.i.d gaussian random variables with parameters $\mu$ and $\sigma$.
\begin{equation}\label{c1pe10}
\ev(X_i X_j) = \int x_i x_j \frac{1}{2\pi\abs{\Sigma}^2}\exp\left(-\frac{(x - \mu)^T\Sigma^{-1}(x - \mu)}{2}\right)dx_i dx_j,
\end{equation}
where $X = (X_i, X_j), \mu = (\mu, \mu)$ and $\Sigma$ is the variance-covariance matrix
of $X$. Since $X_i$ and $X_j$ are independent, $\Sigma = \diag(\sigma^2, \sigma^2)$. Then,
\[
(x - \mu)^T\Sigma^{-1}(x - \mu) = \frac{(x_i - \mu)^2}{\sigma^2} + \frac{(x_j - \mu)^2}{\sigma^2}
\]
and $\abs{\Sigma} = \sigma^4$. If $i \ne j$, equation \eqref{c1pe10} then becomes
\[
E(X_iX_j) = \frac{1}{\sqrt{2\pi\sigma^2}}\int x_i \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)dx_i
            \frac{1}{\sqrt{2\pi\sigma^2}}\int x_j \exp\left(-\frac{(x_j - \mu)^2}{2\sigma^2}\right)dx_j
\]
or
\begin{equation}\label{c1pe11}
E(X_iX_j) = \mu^2, i \ne j
\end{equation}
When $i = j$, $\ev(X_iX_j) = \ev(X_i^2) = \var(X_i) + (\ev(X_i))^2 = \sigma^2 + \mu^2$. Combining
this with equation \eqref{c1pe10} we get
\[
\ev(X_iX_j) = \mu^2 + \sigma^2\delta_{ij}.
\]

Let us now find the expectation of the maximum likelihood estimates.
\begin{equation}\label{c1pe12}
\ev{\mu_{ML}} = \frac{1}{N}\sum_{n=1}^N\ev{x_n} = \mu.
\end{equation}
We also need its variance. As a first step, we need
\[
\var{\mu_{ML}} = \ev[(\mu_{ML} - \mu)^2] = \ev(\mu_{ML})^2 - 2\mu\ev(\mu_{ML}) + \mu^2) = \ev(\mu_{ML})^2 - \mu^2.
\]
In order to evaluate the first term, we need to find $\ev(x_nx_m)$. As they are i.i.d. variables, their
covariance is zero. Therefore, when $n \ne m$,
\[
\ev[(x_n - \mu)(x_m - \mu)] = 0 \Rightarrow \ev(x_nx_m) = \mu^2,
\]
and when $n = m$,
\[
\ev[(x_n - \mu)^2] = \sigma^2 \Rightarrow \ev(x_n^2) = \sigma^2 + \mu^2.
\]
We can combine the previous two equations to get
\begin{equation}\label{c1pe13}
\ev(x_nx_m) = \mu^2 + \sigma^2\delta_{nm}.
\end{equation}
We now proceed to find
\begin{equation}\label{c1pe14}
\ev(\mu^2_{ML}) = \frac{1}{N^2}\sum_{m,n=1}^N\ev(x_nx_m) = \mu^2 + \frac{\sigma^2}{N}.
\end{equation}

The expectation value of $\sigma_{ML}^2$ is
\[
\ev(\sigma_{ML}^2) = \frac{1}{N}\sum_{n=1}^N[\ev(x_n^2) - 2\ev(x_n\mu_{ML}) + \ev(\mu_{ML}^2)]
\]
Using equations \eqref{c1pe14} and \eqref{c1pe13} we get
\begin{eqnarray*}
\ev(\sigma_{ML}^2) &=& \frac{1}{N}\sum_{n=1}^N\left[\mu^2 + \sigma^2 - 2\ev(x_n\mu_{ML}) + \mu^2 + \frac{\sigma^2}{N}\right] \\
 &=& \frac{1}{N}\sum_{n=1}^N\left[2\mu^2 + \sigma^2 + \frac{\sigma^2}{N} - \frac{2}{N}\sum_{m=1}^N\ev(x_nx_m)\right] \\
 &=& \frac{1}{N}\sum_{n=1}^N\left[2\mu^2 + \sigma^2 + \frac{\sigma^2}{N} - \frac{2}{N}\sum_{m=1}^n(\mu^2 + \sigma^2\delta_{mn})\right] \\
 &=& \frac{1}{N}\sum_{n=1}^N \frac{N - 1}{N}\sigma^2
\end{eqnarray*}
so that
\begin{equation}\label{c1pe15}
\ev(\sigma_{ML}^2) = \frac{N - 1}{N}\sigma^2.
\end{equation}

\item Consider the quantity
\[
s^2 = \frac{1}{N}\sum_{n=1}^N (x_n - \mu)^2
\]
then
\[
\ev(s^2) = \frac{1}{N}\sum_{n=1}^n(\ev(x_n^2) - 2\mu\ev(x_n) + \mu^2) = \frac{1}{N}\sum_{n=1}^N(\sigma^2 + \mu^2 - \mu^2) = \sigma^2.
\]

\end{enumerate}
