\chapter{Probability Distributions}\label{c2}
\section{Multinomial variables}\label{c2s1}
A multinomial variable can take one out of $K$ possible values. One can express it as a variable
with a range restricted to $K$ integers, typically $1$ to $K$. Alternatively, we can express it as 
a boolean  $K$ vector where exactly one component is $1$ and the rest are $0$. We choose the latter 
alternative. We use the notation customary in linear algebra to denote the boolean vector with just 
the $k$-th component $1$ and the remaining ones zero by $e_k$.

Let a multinomial variable $X$ take a value $k$ with probability $\mu_k$. Then 
\[
\sum_{k=1}^K \mu_k = 1
\]
and 
\begin{equation}\label{c2s1e1}
p(X = e_k|\mu) = \prod_{k=1}^K \mu_k^{x_k},
\end{equation}
where $x_k$ is the $k$th component of the boolean $K$-vector representing the state
$X = k$. Every component of the vector contributes a $1$ to the product except the
one with $x_k = 1$, which contributes $\mu_k$. Therefore,
\[
p(X = e_k|\mu) = \mu_k
\]
and hence
\begin{equation}\label{c2s1e2}
\sum_{k=1}^K`p(X = e_k|\mu) = 1.
\end{equation}

The expectation of a multinomial random variable is
\begin{equation}\label{c2s1e3}
\ev(X|\mu) = \sum_{k=1}^K e_kp(X = e_k|\mu) = (\mu_1, \ldots, \mu_K) = \mu.
\end{equation}
A dataset $\mathcal{D}$ of $N$ i.i.d. multinomial random variables has a prior probability
\[
p(\mathcal{D}|\mu) = \prod_{n=1}^Np(X_n = e_k|\mu) = \prod_{n=1}^N\prod_{k=1}^K \mu_k^{(x_n)_k}.
\]
In this equation, the data set has vectors ${x_1, \ldots, x_N}$, all in $\{0, 1\}^K$ and whose
$k$-th component is denoted by $(x_n)_k$. We can simplify the previous equation as
\[
p(\mathcal{D}|\mu) = \prod_{k=1}^K \mu_k^{\sum_{n=1}^N (x_n)_k}.
\]
The sum 
\[
\sum_{n=1}^N (x_n)_k
\]
is the number of $x_n$ whose $k$th component is $1$, that is the number of $x_n = e_k$. Let
us call this number $m_k$, Then
\begin{equation}\label{c2s1e4}
p(\mathcal{D}|\mu) = \prod_{k=1}^K \mu_k^{m_k}.
\end{equation}
It is clear that 
\begin{equation}\label{c2s1e5}
\sum_{k=1}^K m_k = N.
\end{equation}

We could have arrived at equation \eqref{c2s1e4} with a much simpler analysis. If the probability
that a multinomial random variable takes a value $k$ is $\mu_k$ and if a data set of $N$ such
variables has $m_k$ variables taking the value $k$ then
\[
p(\mathcal{D}|\mu) = \prod_{k=1}^K \mu_k^{m_k}.
\]
We took the longer route only to demonstrate how to work with boolean $K$-vectors.

In order to find the maximum-likelihood estimate of the $\mu$, we maximize the logarithm of
equation \eqref{c2s1e4} subject to the constraint $\sum_{k=1}^K \mu_k = 1$. If $\lambda$ is
the Lagrange's undermined multiplier then we consider the function
\[
f(\mu_k) = \sum_{k=1}^Lm_k\ln{\mu_k} + \lambda\left(\sum_{k=1}^K \mu_k - 1\right)
\]
and its derivative
\[
\frac{df}{d\mu_k} = \frac{m_k}{\mu_k} + \lambda.
\]
The maximum likelihood estimate of $\mu$ is thus,
\begin{equation}\label{c2s1e6}
\mu_{ML} = -\frac{1}{\lambda}(m_1, \ldots, m_K).
\end{equation}
The multiplier $\lambda$ can be found by substituting the previous equation in the equation
of constraint. Thus,
\[
\sum_{k=1}^K \mu_k = -\sum_{k=1}^K \frac{m_k}{\lambda} = -\frac{1}{\lambda}\sum_{k=1}^K m_k = 1.
\]
From equation \eqref{c2s1e5} we have
\[
\lambda = -\frac{1}{N}.
\]
Therefore the maximum-likelihood estimate of \eqref{c2s1e6} becomes
\begin{equation}\label{c2s1e7}
\mu_{ML} = \left(\frac{m_1}{N}, \ldots, \frac{m_k}{N}\right).
\end{equation}

\section{Problems}\label{c2p}
\begin{enumerate}
\item The Bernoulli distribution is defined as $\mathrm{Bern}(x|\mu) = \mu^x(1 - \mu)^x$. The 
random variable can take only two values, $0$ and $1$. Clearly, $p(X=0|\mu) = 1 - \mu$ and 
$p(X=1|\mu) = \mu$ so that
\[
\sum_x p(x|\mu) = 1.
\]
The expectation value of $X$ is
\[
\ev(X) = \sum_x xp(x|\mu) = 0\cdot(1 - \mu) + 1\cdot\mu = \mu.
\]
The variance of $X$ is
\[
\var(X) = \sum_x \ev(x - \mu)^2 = (0 - \mu)^2(1 - \mu) + (1 - \mu)^2\mu = \mu(1 - \mu).
\]

\item Given that
\[
p(x|\mu) = \left(\frac{1-\mu}{2}\right)^{(1-x)/2}\left(\frac{1+\mu}{2}\right)^{(1+x)/2},
\]
where $x \in {-1, 1}$ and $\mu \in [0, 1]$. Then $p(X=-1|\mu) = (1 - \mu)/2$ and $p(X=1|\mu)
= (1 + \mu)/2$ so that 
\[
\sum_x p(x|\mu) = 1.
\]
The expectation value of $X$ is
\[
\ev(X) = \sum_x xp(x|\mu) = -1\cdot\frac{1 - \mu}{2} + 1\cdot\frac{1 + \mu}{2} = \mu.
\]
We also find
\[
\ev(X^2) = \frac{1 - \mu}{2} + \frac{1 + \mu}{2} = 1
\]
so that $\var(X) = 1 - \mu^2$. We verify the variance using the other form,
\[
\var(X) = (-1 - \mu)^2\frac{1 - \mu}{2} + (1 - \mu)^2\frac{1 + \mu}{2} = 1 - \mu^2.
\]

\item We can show that the binomial distribution is normalized by evaluating the sum,
\[
\sum_{m=0}^N\binom{N}{m}\mu^m(1 - \mu)^{N-m} = (\mu + 1 - \mu)^N = 1.
\]
We used the binomial theorem to get the first equality.

\item Consider the expression,
\[
\sum_{k=0}^N \binom{N}{k}\mu^k(1 - \mu)^{N-k} = 0.
\]
Differentiate it with respect to $\mu$. Recall that $\mu \in [0, 1]$ so that the differentiation 
is well defined.
\[
\sum_{k=0}^N\binom{N}{k}k\mu^{k-1}(1 - \mu)^{N-k} - \sum_{k=0}^N\binom{N}{k}(N-k)\mu^k(1 - \mu)^{N-k-1} = 0.
\]
Multiply throughout by $\mu$ to get
\[
\sum_{k=0}^N\binom{N}{k}\mu^k(1 - \mu)^{N-k} = \mu N\sum_{k=0}^{N-1}\binom{N-1}{k}\mu^k(1 - \mu)^{N-k-1}.
\]
The left hand side is $\bar{X}$ while the right hand side is $\mu N$.

\item Consider the product
\[
\Gamma(a)\Gamma(b) = \int_0^\infty e^{-x}x^{a-1}dx\int_0^\infty e^{-y}y^{b-1}dy = \iint_{Q_1}e^{-x-y} x^{a-1}y^{b-1}dxdy,
\]
where $Q_1$ is the first quadrant. To evaluate this integral, introduce the variables $x = zt$
and $y = z(1 - t)$ or $z = x + y$ and $t = x/(x + y)$. The variable $z$ ranges from $0$ to 
$\infty$ while $t$ goes from $0$ to $1$. The jacobian of the transformation is
\[
\frac{\partial(x, y)}{\partial(z, t)} = \begin{vmatrix} x_z & x_t \\ y_z & y_t \end{vmatrix}
= \begin{vmatrix} t & z \\ 1 - t & -z \end{vmatrix} = -z.
\]
Therefore, 
\[
\Gamma(a)\Gamma(b) = \int_0^\infty\int_0^1 e^{z} z^{a-1}t^{a-1}z^{b-1}(-t)^{b-1}\abs*{\frac{\partial(x, y}{\partial(z, t)}}dzdt.
\]
The $z$ and the $t$ integrals factor out, so that
\[
\Gamma(a)\Gamma(b) = \int_0^\infty e^z z^{a+b-1}dz\int_0^1 t^{a-1}(1 - t)^{b- 1}dt = \Gamma(a+b)B(a, b).
\]

\item The mean of beta distribution is
\begin{eqnarray*}
\ev(X) &=& \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x B(x|a, b)dx \\
 &=& \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1x^a(1 - x)^{b-1}dx \\
 &=& \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)} \\
 &=& \frac{a}{a+b}.
\end{eqnarray*}
Likewise,
\begin{eqnarray*}
\ev(X^2) &=& \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^2 B(x|a, b)dx \\
 &=& \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1x^{a+2}(1 - x)^{b-1}dx \\
 &=& \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)} \\
 &=& \frac{a(a+1)}{(a+b)(a+b+1)}.
\end{eqnarray*}
Therefore,
\[
\var{X} = \frac{a(a+1)}{(a+b)(a+b+1)} - \frac{a^2}{(a+b)^2} = \frac{ab}{(a+b)^2(a+b+1)}.
\]

The mode of the distribution is the maximum of $\mathrm{Beta}(X|a, b)$. Its derivative
with respect to $x$ is
\[
\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\left((a-1)x^{a-2}(1-x)^{b-1} - (b-1)x^{a-1}(1-x)^{b-2}\right)
\]
The extremum is found by setting it equal to zero, which gives
\[
(a-1)(1 - x) = (b - 1)x
\]
or
\[
x = \frac{a-1}{a+b-2}.
\]

We can use the following R code to generate plots of beta distributions for various
values of $a$ and $b$.
\begin{lstlisting}[language=R, frame=single]
x <- seq(from = 0, to = 1, by = 0.01)
b.1 <- dbeta(x, 0.1, 0.1)
b.2 <- dbeta(x, 1, 1)
b.3 <- dbeta(x, 2, 3)
b.4 <- dbeta(x, 8, 4)
plot(x, 
     b.1, 
     type = "l", 
     xlab = "x", 
     ylab = "B(a, b)", 
     main = "Beta distributions")
lines(x, b.2, col = 2, lty = 2)
lines(x, b.3, col = 3, lty = 3)
lines(x, b.4, col = 4, lty = 4)
legend(locator(1), 
       legend = c("(0.1,0.1)","(1,1)","(2,3)","(8,4)"), 
       col = c(1,2,3,4), 
       lty = c(1,2,3,4), 
       cex = 0.7, 
       bty = "n")
\end{lstlisting}

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{c2f1}
\end{center}
\end{figure}

\item Let $X$ be a binomial random variable with probability mass function
\[
p(X = m|\mu) = \binom{N}{n}\mu^n (1 - \mu)^{N-n}.
\]
Let the parameter $\mu$ be described by a prior distribution
\begin{equation}\label{c2pe1}
\mathrm{Beta}(\mu|a, b) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1 - \mu)^{b-1}.
\end{equation}
The observed data of $m + l$ realizations of $X$ has $x = 1$ $m$ times and $x = 0$ $l$ times.
Therefore, the likelihood function is
\[
p(\mathcal{D}|\mu) = \prod_{i=1}^{m}p(X=1|\mu)\prod_{j=1}^lp(X=0|\mu) = \mu^m(1 - \mu)^l.
\]
The posterior distribution of $\mu$ given the data $\mathcal{D}$ is
\[
p(\mu|D) \propto p(\mathcal{D}|\mu)\mathrm{Beta}(\mu|a, b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{m+a-1}(1-\mu)^{l+b-1}.
\]
The constant of proportionality is
\[
\frac{\Gamma(m+l+a+b)}{\Gamma(m+a)\Gamma(b+l)}
\]
so that
\[
p(\mu|\mathcal{D}) = \frac{\Gamma(m+l+a+b)}{\Gamma(m+a)\Gamma(b+l)}\mu^{m+a-1}(1-\mu)^{l+b-1}.
\]
The mean of $\mu$ conditioned on $\mathcal{D}$ is the posterior mean
\begin{equation}\label{c2pe2}
\mu_{post} = \frac{m+a}{m+a+l+b}.
\end{equation}

The log-likelihood function is
\[
\ln p(\mathcal{D}|\mu) = m\ln\mu + l\ln(1 - \mu)
\]
and its derivative is
\[
\frac{m}{\mu} - \frac{l}{1 - \mu}
\]
and its extremum is 
\begin{equation}\label{c2pe3}
\mu_{ML} = \frac{m}{m+l}.
\end{equation}
The mean of the prior distribution is that of equation \eqref{c2pe1}
\begin{equation}\label{c2pe4}
\mu_{prior} = \frac{a}{a+b}.
\end{equation}

Let us assume, for the moment that the maximum likelihood estimate is greater than the prior estimate.
That is,
\[
\mu_{ML} - \mu_{prior} > 0.
\]
This is possible if and only if 
\begin{equation}\label{c2pe5}
bm > al.
\end{equation}
If equation \eqref{c2pe5} is true then it is easy to verify that $\mu_{ML} > \mu_{post} > \mu_{prior}$. 
If \eqref{c2pe5} is not true then $\mu_{prior} \ge \mu_{post} \ge \mu_{ML}$.

\item Let $X$ and $Y$ be random variables with joint distribution $p(x, y)$. Then,
\[
\ev_x(X|Y) = \int xp(x|y)dx
\]
and 
\begin{eqnarray*}
\ev_y(\ev_x(X|Y)) &=& \int p(y)\ev_x(X|Y)dy \\
 &=& \iint xp(y)p(x|y)dxdy \\
 &=& \iint xp(x, y)dxdy \\
 &=& \ev(X).
\end{eqnarray*}

For sake of notational simplicity, let us denote $\ev_x(X|Y) = \phi(y)$. Then, 
\begin{equation}\label{c2pe6}
\var_y(\ev_x(X|Y)) = \var_y(\phi(y)) = \ev_y(\phi^2(y)) - (\ev_y\phi(y))^2
\end{equation}
Now, $\var_x(X|Y) = \ev_x(X^2|Y) - (\ev_x(X|Y))^2 = \ev_x(X^2|Y) - \phi^2(y)$ so that 
\[
\ev_y(\var_x(X|Y)) = \ev_y(\ev_x(X^2|Y)) - \ev_y(\phi^2(y))
\]
and hence
\[
\begin{split}
\var_y(\ev_x(X|Y)) + \ev_y(\var_x(X|Y)) =& \ev_y(\phi^2(y)) - (\ev_y\phi(y))^2 + \\
 & \ev_y(\ev_x(X^2|Y)) - \ev_y(\phi^2(y)) 
\end{split}
\]
and finally
\begin{equation}\label{c2pe7}
\var_y(\ev_x(X|Y)) + \ev_y(\var_x(X|Y)) = \ev_y(\ev_x(X^2|Y)) - (\ev_y\phi(y))^2 
\end{equation}
Now,
\begin{eqnarray*}
\ev_y(\ev_x(X^2|Y)) &=& \iint x^2 p(x|y)p(y)dxdy = \var(X) \\
\ev_y\phi(y) &=& \iint xp(x|y)p(y)dy = \ev(X)
\end{eqnarray*}
Using these in equation \eqref{c2pe7} we get the desired result.

\item We use the following result,
\begin{equation}\label{c2pe8}
\int_0^a x^{m-1}(a - x)^{n-1}dx = a^{m+n-1}B(m, n).
\end{equation}
To obtain it, we write the integral as
\[
a^{m+n-2}\int_0^a \left(\frac{x}{a}\right)^{m-1}\left(1 - \frac{x}{a}\right)^{n-1}ad\left(\frac{x}{a}\right).
\]
A transformation $x/a \mapsto y$ yields the result. The Dirichlet distribution is proportional
to 
\[
D = \prod_{k=1}^K\mu_k^{\alpha_k - 1}.
\]
subject to the constraint 
\begin{equation}\label{c2pe9}
\sum_{k=1}^K\mu_k = 1.
\end{equation}
Therefore, the distribution is proportional to
\[
D = \prod_{k=1}^{K-1}\mu_k^{\alpha_k - 1}(1 - \mu_1 - \cdots - \mu_{K-1})^{\alpha_K-1}
\]
The variables $\mu_k$ vary over $[0, 1]$. However, the constraint of equation \eqref{c2pe9} reduce the
region of integration from a hypercube to a $K-1$ simplex formed by the first quandrant and the plane whose 
equation is \eqref{c2pe9}. In order to find the normalization constant we must evaluate the integral
\begin{equation}\label{c2pe10}
I = \int_0^1\int_0^{1-\mu_1}\cdots\int_0^{1-\mu_1-\cdots-\mu_{K-2}}D(\mu_1, \ldots, \mu_{K-1})d\mu_1\cdots d\mu_{K-1}.
\end{equation}
The $\mu_{K-1}$ integral is
\begin{eqnarray*}
\int_0^{1-\mu_1-\cdots-\mu_{K-2}}\mu_{K-1}^{\alpha_K - 1}(1 - \mu_1 - \cdots - \mu_{K-1})^{\alpha_K-1}d\mu_{K-1} &=& \\
 (1-\mu_1-\cdots-\mu_{K-2})^{\alpha_K+\alpha_{K-1}-1}B(\alpha_K, \alpha_{K-1}) & &
\end{eqnarray*}
The integrals over the other variables are evaluated similarly so that
\[
I = B(\alpha_{K-1},\alpha_K)B(\alpha_{K-2},\alpha_{K}+\alpha_{K-1})\cdots B(\alpha_1,\alpha_{K}+\cdots+\alpha_2).
\]
Using the relation,
\[
B(m, n) = \frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)},
\]
we get
\begin{eqnarray*}
I &=& \frac{\Gamma(\alpha_{K-1})\Gamma(\alpha_K)}{\Gamma(\alpha_{K}+\alpha_{K-1})} \\
  & &  \frac{\Gamma(\alpha_{K-2})\Gamma(\alpha_K+\alpha_{K-1})}{\Gamma(\alpha_K+\alpha_{K-1}+\alpha_{K-2})}\cdots \\
  & &  \frac{\Gamma(\alpha_1)\Gamma(\alpha_K+\alpha_{K-1}+\cdots+\alpha_2)}{\Gamma(\alpha_K+\alpha_{K-1}+\cdots+\alpha_{1})}
\end{eqnarray*}
Cancelling common terms,
\[
I = \frac{\Gamma(\alpha_K)\Gamma(\alpha_{K-1})\cdots\Gamma(\alpha_1)}{\Gamma(\alpha_K+\alpha_{K-1}+\cdots+\alpha_1)}
\]
If we write $\alpha_0 = \alpha_1 + \cdots + \alpha_K$ then the normalization constant for the
Dirichlet distribution is
\[
\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}.
\]

\item We start with the Dirichlet distribution
\[
\mathrm{Dir}(\mu|\alpha) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{\alpha_k - 1},
\]
where $\mu, \alpha \in \sor^K$ and
\begin{eqnarray*}
\sum_{i=1}^K \mu_k &=& 1 \\
\sum_{i=1}^K \alpha_k &=& \alpha_0
\end{eqnarray*}
The mean of the distribution is
\begin{eqnarray*}
\ev(\mu_j) &=& \int\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{\alpha_k + \delta_{jk} - 1}d\mu \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\frac{\Gamma(\alpha_1)\cdots\Gamma(\alpha_j+1)\cdots\Gamma(\alpha_K)}{\Gamma(\alpha_0 + 1)} \\
 &=& \frac{\alpha_j}{\alpha_0}
\end{eqnarray*}
In order to find the variance, we need
\begin{eqnarray*}
\ev(\mu_j^2) &=& \int\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{\alpha_k + 2\delta_{jk} - 1}d\mu \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\frac{\Gamma(\alpha_1)\cdots\Gamma(\alpha_j+2)\cdots\Gamma(\alpha_K)}{\Gamma(\alpha_0 + 2)} \\
 &=& \frac{\alpha_j(\alpha_j+1)}{\alpha_0(\alpha_0+1)}
\end{eqnarray*}
Therefore,
\[
\var(\alpha_j) = \frac{\alpha_j(\alpha_j+1)}{\alpha_0(\alpha_0+1)} - \frac{\alpha_j^2}{\alpha_0^2} = \frac{\alpha_j(\alpha_0 - \alpha_j)}{\alpha_0^2(\alpha_0+1)}.
\]
Note that the definition of $\alpha_0$ guarantees us that the variance is non-negative. In order
to find the covariance, we need
\begin{eqnarray*}
\ev(\mu_i\mu_j) &=& \int\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{\alpha_k + \delta_{ik} + \delta_{jk} - 1}d\mu \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\frac{\Gamma(\alpha_1)\cdots\Gamma(\alpha_i+1)\cdots\Gamma(\alpha_j+1)\cdots\Gamma(\alpha_K)}{\Gamma(\alpha_0 + 2)} \\
 &=& \frac{\alpha_i\alpha_j}{\alpha_0(\alpha_0+1)}
\end{eqnarray*}
so that
\[
\cov(\mu_i\mu_j) = \frac{\alpha_i\alpha_j}{\alpha_0(\alpha_0+1)} - \frac{\alpha_i\alpha_j}{\alpha_0^2} = -\frac{\alpha_i\alpha_j}{\alpha_0^2(\alpha_0+1)}.
\]

\item Recall that the derivative of $a^x$ with respect to $x$ is $a^x\ln a$. Therefore,
\[
\frac{\partial}{\partial\alpha_j}\prod_{k=1}^K \mu_k^{\alpha_k-1} = \ln\mu_j\prod_{k=1}^K\mu_k^{\alpha_k-1}.
\]
Now,
\begin{eqnarray*}
\ev(\ln\mu_j) &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\int\ln\mu_j\prod_{k=1}^K\mu_k^{\alpha_k-1}d\mu \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\int\frac{\partial}{\partial\alpha_j}\prod_{k=1}^K\mu_k^{\alpha_k-1}d\mu \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\frac{\partial}{\partial\alpha_j}\int\prod_{k=1}^K\mu_k^{\alpha_k-1}d\mu \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\frac{\partial}{\partial\alpha_j}\frac{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}{\Gamma(\alpha_0)} \\
 &=& \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_j)}\frac{\partial}{\partial\alpha_j}\frac{\Gamma(\alpha_j)}{\Gamma(\alpha_0)} \\
 &=& \frac{1}{\Gamma(\alpha_j)}\frac{\partial}{\partial\alpha_j}\Gamma(\alpha_j) - \frac{1}{\Gamma(\alpha_0)}\frac{\partial}{\partial\alpha_j}\Gamma(\alpha_0) \\
 &=& \frac{\partial}{\partial\alpha_j}\ln\Gamma(\alpha_j) - \frac{\partial}{\partial\alpha_j}\ln\Gamma(\alpha_0) \\
 &=& \psi(\alpha_j) - \psi(\alpha_0),
\end{eqnarray*}
where 
\[
\psi(\alpha) = \frac{d}{d\alpha}\ln\Gamma(\alpha)
\]
is the digamma function.

\item The uniform distribution is defined by
\[
\mathrm{Unif}(x|a, b) = \frac{1}{b - a}, x \in [a, b].
\]
It is easy to confirm that it is normalized, for
\[
\int_a^b \mathrm{Unif}(x|a, b)dx = \frac{1}{b-a}\int_a^b dx = 1.
\]
The mean value of $x$ is
\[
\ev(x) = \int_a^b x\mathrm{Unif}(x|a, b)dx = \int_a^b \frac{x}{b-a}dx = \frac{1}{2(b-a)}(b^2 - a^2) = \frac{a+b}{2}.
\]
The mean value of $x^2$ is
\[
\ev(x^2) = \int_a^b\frac{x^2}{b-a}dx = \frac{1}{3(b-a)}(b^3-a^3) = \frac{a^2 + ab + b^2}{3}
\]
so that 
\[
\var(x) = \frac{a^2 + ab + b^2}{3} - \frac{(a+b)^2}{4} = \frac{(b - a)^2}{12}.
\]
\end{enumerate}

